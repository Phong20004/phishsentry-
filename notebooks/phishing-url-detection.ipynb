{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12281144,"sourceType":"datasetVersion","datasetId":7739659}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom urllib.parse import urlparse\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom xgboost import XGBClassifier\nimport joblib\nfrom tqdm import tqdm\n\ndef normalize_url(url):\n    return url if url.startswith(\"http\") else \"http://\" + url\n\ndef strip_scheme_www(url):\n    url = normalize_url(url)\n    parsed = urlparse(url)\n    domain = parsed.netloc.replace(\"www.\", \"\")\n    path = parsed.path or \"\"\n    query = f\"?{parsed.query}\" if parsed.query else \"\"\n    return domain + path + query\n\nclass URLFeatureExtractor(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.suspicious_keywords = [\n            'login', 'secure', 'update', 'verify', 'account', 'bank', 'signin', 'submit',\n            'paypal', 'ebay', 'confirm', t'wp', 'mail', 'admin', '88', '365', 'bet', '68', '86'\n        ]\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        features = []\n        for url in tqdm(X, desc=\"Đang trích đặc trưng\"):\n            features.append([\n                len(url),\n                url.count('.'),\n                url.count('-'),\n                int(bool(re.search(r'(\\d{1,3}\\.){3}\\d{1,3}', url))),\n                sum(1 for word in self.suspicious_keywords if word in url.lower())\n            ])\n        return np.array(features)\n\ndef load_datasets():\n    print(\"Đang tải và gộp dữ liệu...\")\n    df1 = pd.read_csv(\"/kaggle/input/dataset/openphish.csv\")[['url']].assign(label=1)\n    df2 = pd.read_csv(\"/kaggle/input/dataset/urlhaus.csv\")[['url']].assign(label=1)\n    df3 = pd.read_csv(\"/kaggle/input/dataset/phishtank.csv\")[['url']].assign(label=1)\n\n    df4 = pd.read_csv(\"/kaggle/input/dataset/legit.csv\", header=None, names=[\"index\", \"domain\"])\n    df4['url'] = \"http://\" + df4['domain'].astype(str)\n    df4 = df4[['url']].assign(label=0)\n\n    df_phish = pd.concat([df1, df2, df3], ignore_index=True).drop_duplicates(subset='url')\n    df_legit = df4.drop_duplicates(subset='url')\n\n    min_len = min(len(df_phish), len(df_legit))\n    df_phish = df_phish.sample(n=min_len, random_state=42)\n    df_legit = df_legit.sample(n=min_len, random_state=42)\n\n    print(f\"Dữ liệu sau cân bằng: {len(df_phish)} lừa đảo, {len(df_legit)} hợp lệ\")\n\n    df = pd.concat([df_phish, df_legit], ignore_index=True)\n    df['url'] = df['url'].astype(str).apply(strip_scheme_www)\n    return df\n    \ndef train_model():\n    df = load_datasets()\n    X = df['url']\n    y = df['label']\n\n    print(\"Trích xuất đặc trưng...\")\n    vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(3, 5), max_features=5000)\n    feature_union = FeatureUnion([\n        ('tfidf', vectorizer),\n        ('custom', URLFeatureExtractor())\n    ])\n    X_features = feature_union.fit_transform(X)\n\n    print(\"Chia tập train/test...\")\n    X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=0.2, random_state=42)\n\n    print(\"Huấn luyện mô hình XGBoost...\")\n    model = XGBClassifier(\n        tree_method='hist',\n        device='cuda',\n        max_depth=30,\n        n_estimators=200,\n        use_label_encoder=False,\n        eval_metric='logloss',\n        verbosity=1\n    )\n    model.fit(X_train, y_train)\n\n    print(\"Đánh giá mô hình:\")\n    y_pred = model.predict(X_test)\n    print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n    print(classification_report(y_test, y_pred))\n\n    print(\"Đang lưu model...\")\n    joblib.dump(model, 'model.pkl')\n    joblib.dump(feature_union, 'vectorizer.pkl')\n    print(\"Đã lưu model.pkl và vectorizer.pkl\")\n\nif __name__ == \"__main__\":\n    train_model()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T00:44:23.540921Z","iopub.execute_input":"2025-06-26T00:44:23.541218Z"}},"outputs":[{"name":"stdout","text":"Đang tải và gộp dữ liệu...\n","output_type":"stream"}],"execution_count":null}]}